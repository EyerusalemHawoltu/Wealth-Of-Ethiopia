ETHICS.md

In my web scraping project, I emphasized adherence to the target website's terms of service by carefully reviewing them before commencing data collection. This proactive measure ensures that my activities align with the website's guidelines, significantly reducing legal risks.

Checking Site Permissions: Before starting the scraping process, we inspected the target websiteâ€™s robots.txt file to verify that our requests complied with their specified rules. This step is crucial for avoiding access to restricted content.

Incorporating Request Delays: Although our scraping operation focuses on a single page, we implemented delay intervals using time.sleep() to mitigate the risk of sending excessive requests that could burden the server. This practice is essential for preserving the website's performance and enhancing user experience.

Commitment to Responsible Data Collection: The data gathered through this scraping effort is intended exclusively for educational purposes and adheres to fair use principles. We prioritized collecting publicly available information while respecting the website's terms of service, reinforcing our commitment to ethical data practices.
